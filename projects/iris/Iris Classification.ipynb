{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll be working with one of the most well-known machine learning datasets - the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/Iris) hosted at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html). Our goal is to train a network to identify a species of iris based on the flower's sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "The dataset contains 50 data points for each of the three species, *Iris setosa, Iris versicolour*, and *Iris Virginica* for a total of 150 data points. \n",
    "![Iris types](img/iris_types.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data into a `pandas` dataframe as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of records in each column to ensure all of our datapoints are complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the data type for each column like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning problems, it can be helpful to try and visualize the data where possible in order to get a feel for the problem. The `seaborn` library has some great tools for this.\n",
    "\n",
    "**Caution:** You may not have `seaborn` installed on your machine. If this is the case, use the `pip` installer from your shell (Mac OSX/Linux): `pip install seaborn`. If you're on Windows, you won't be able to install `scipy` using `pip`. You'll have to use `conda` to install the package or manually download and install a wheel yourself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the relationship between two features and the target classes using `seaborn`'s `FacetGrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.FacetGrid(iris, hue=\"Species\", size=6) \\\n",
    "   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use `pairplot` to do this for all combinations of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can see that *Iris setosa* is linearly separable from the others in all feature pairs. This could prove useful for the design of our network classifier.\n",
    "\n",
    "Now that we've loaded our data and we know how it's structured, it's up to *you* to create a neural network classifier! I've given you some code to branch off of below. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 cost: 0.2048\n",
      "Epoch 0002 cost: 0.1534\n",
      "Epoch 0003 cost: 0.1380\n",
      "Epoch 0004 cost: 0.1444\n",
      "Epoch 0005 cost: 0.1445\n",
      "Epoch 0006 cost: 0.1965\n",
      "Epoch 0007 cost: 0.1807\n",
      "Epoch 0008 cost: 0.2030\n",
      "Epoch 0009 cost: 0.1620\n",
      "Epoch 0010 cost: 0.1128\n",
      "Epoch 0011 cost: 0.1207\n",
      "Epoch 0012 cost: 0.1080\n",
      "Epoch 0013 cost: 0.1536\n",
      "Epoch 0014 cost: 0.1600\n",
      "Epoch 0015 cost: 0.0969\n",
      "Epoch 0016 cost: 0.0941\n",
      "Epoch 0017 cost: 0.1228\n",
      "Epoch 0018 cost: 0.1264\n",
      "Epoch 0019 cost: 0.0844\n",
      "Epoch 0020 cost: 0.0985\n",
      "Epoch 0021 cost: 0.1010\n",
      "Epoch 0022 cost: 0.1238\n",
      "Epoch 0023 cost: 0.0665\n",
      "Epoch 0024 cost: 0.0564\n",
      "Epoch 0025 cost: 0.1091\n",
      "Epoch 0026 cost: 0.0813\n",
      "Epoch 0027 cost: 0.0553\n",
      "Epoch 0028 cost: 0.0731\n",
      "Epoch 0029 cost: 0.0389\n",
      "Epoch 0030 cost: 0.0582\n",
      "Epoch 0031 cost: 0.0482\n",
      "Epoch 0032 cost: 0.0686\n",
      "Epoch 0033 cost: 0.0887\n",
      "Epoch 0034 cost: 0.0941\n",
      "Epoch 0035 cost: 0.0393\n",
      "Epoch 0036 cost: 0.0395\n",
      "Epoch 0037 cost: 0.0316\n",
      "Epoch 0038 cost: 0.0788\n",
      "Epoch 0039 cost: 0.0329\n",
      "Epoch 0040 cost: 0.0411\n",
      "Epoch 0041 cost: 0.0478\n",
      "Epoch 0042 cost: 0.0408\n",
      "Epoch 0043 cost: 0.0477\n",
      "Epoch 0044 cost: 0.0350\n",
      "Epoch 0045 cost: 0.0377\n",
      "Epoch 0046 cost: 0.0416\n",
      "Epoch 0047 cost: 0.0687\n",
      "Epoch 0048 cost: 0.0335\n",
      "Epoch 0049 cost: 0.0509\n",
      "Epoch 0050 cost: 0.0427\n",
      "\n",
      "The model correctly identified 93.33% of the test data.\n",
      "\n",
      "Model data saved to tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# This cell can be run independently of the ones above it.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Path for saving model data\n",
    "model_path = 'tmp/model.ckpt'\n",
    "\n",
    "# Hyperparameters \n",
    "learn_rate = .5\n",
    "batch_size = 10\n",
    "epochs = 50\n",
    "\n",
    "# Load the data into dataframes\n",
    "# There is NO OVERLAP between the training and testing data\n",
    "# Take a minute to remember why this should be the case!\n",
    "iris_train = pd.read_csv('data/iris_train.csv', dtype={'Species': 'category'})\n",
    "iris_test = pd.read_csv('data/iris_test.csv', dtype={'Species': 'category'})\n",
    "test_features = iris_test.as_matrix()[:,:4]\n",
    "test_targets = pd.get_dummies(iris_test.Species).as_matrix()\n",
    "\n",
    "# Create placeholder for the input tensor (input layer):\n",
    "# Our input has four features so our shape will be (none, 4)\n",
    "# A variable number of rows and four feature columns.\n",
    "x = tf.placeholder(tf.float32, [None, 4])\n",
    "\n",
    "# Outputs will have 3 columns since there are three categories\n",
    "# This placeholder is for our targets (correct categories)\n",
    "# It will be fed with one-hot vectors from the data\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# The baseline model will consist of a single softmax layer with \n",
    "# weights W and bias b\n",
    "# Because these values will be calculated and recalculated\n",
    "# on the fly, we'll declare variables for them.\n",
    "# We use a normal distribution to initialize our matrix with small random values\n",
    "W = tf.Variable(tf.truncated_normal([4, 3], stddev=0.1))\n",
    "\n",
    "# And an initial value of zero for the bias.\n",
    "b = tf.Variable(tf.zeros([3]))  \n",
    "\n",
    "# We define our simple model here\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#=================================================================\n",
    "# And our cost function here (make sure only one is uncommented!)|\n",
    "#=================================================================\n",
    "\n",
    "# Mean Squared Error\n",
    "cost = tf.reduce_mean(tf.squared_difference(y_, y))\n",
    "\n",
    "# Cross-Entropy\n",
    "#cost = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "#\n",
    "#=================================================================\n",
    "\n",
    "# Gradient descent step\n",
    "train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "# Start a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize all of the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Operation for saving all variables\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.\n",
    "        num_batches = int(iris_train.shape[0]/batch_size)\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            # Randomly select <batch_size> samples from the set (with replacement)\n",
    "            batch = iris_train.sample(n=batch_size)\n",
    "\n",
    "            # Capture the x and y_ data\n",
    "            batch_features = batch.as_matrix()[:,:4]\n",
    "\n",
    "            # get_dummies turns our categorical data into one-hot vectors\n",
    "            batch_targets = pd.get_dummies(batch.Species).as_matrix()\n",
    "\n",
    "            # Run the training step using batch_features and batch_targets\n",
    "            # as x and y_, respectively and capture the cost at each step\n",
    "            _, c = sess.run([train_step, cost], feed_dict={x:batch_features, y_:batch_targets})\n",
    "\n",
    "            # Calculate the average cost for the epoch\n",
    "            avg_cost += c/num_batches\n",
    "\n",
    "        # Print epoch results\n",
    "        print(\"Epoch %04d cost: %s\" % (epoch + 1, \"{:.4f}\".format(avg_cost)))\n",
    "    \n",
    "    # If our model's most likely classification is equal to the one-hot index\n",
    "    # add True to our correct_prediction tensor\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "    # Cast the boolean variables as floats and take the mean.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Calculate the percentage of correct answers using the test data\n",
    "    score = sess.run(accuracy, feed_dict={x: test_features, y_: test_targets}) * 100\n",
    "    print(\"\\nThe model correctly identified %s of the test data.\" % \"{:.2f}%\".format(score))\n",
    "\n",
    "    # Save the model data\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"\\nModel data saved to %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Womp womp](http://www.priceisrightfailhorn.com/). See if you can change the hyperparameters (learning rate, batch size, num_batches, etc.), the cost function (uncomment the one you'd like to use) or the structure of the network itself to yield better results!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
